{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install memory-profiler\n",
    "%pip install timeout-decorator\n",
    "%pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "import networkx as nx\n",
    "from timeout_decorator import timeout\n",
    "from queue import PriorityQueue\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tracemalloc\n",
    "import os\n",
    "import ast\n",
    "import psutil\n",
    "import time\n",
    "import threading\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determinando tempo máximo de 30min\n",
    "MAX_TIME_SEG = 1800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criação do dataset com base no nome dos arquivos, e posterior ordenação de acordo com nós"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_order_ds(file_path, sort_column=\"Nós\"):\n",
    "    \"\"\"\n",
    "    Lê um arquivo de dataset delimitado por tabulação e o ordena com base em uma coluna específica.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - file_path (str): Caminho para o arquivo a ser lido.\n",
    "    - sort_column (str): Nome da coluna pela qual o dataset será ordenado. Padrão: 'Nós'.\n",
    "    \n",
    "    Retorna:\n",
    "    - dados_limiares (DataFrame): DataFrame ordenado.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dados_limiares = pd.read_csv(file_path, delimiter='\\t')\n",
    "        \n",
    "        if sort_column not in dados_limiares.columns:\n",
    "            raise KeyError(f\"A coluna '{sort_column}' não foi encontrada no arquivo.\")\n",
    "        \n",
    "        dados_limiares = dados_limiares.sort_values(by=sort_column)\n",
    "        \n",
    "        print(dados_limiares)\n",
    "        return dados_limiares\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erro: O arquivo '{file_path}' não foi encontrado.\")\n",
    "    \n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Erro: O arquivo '{file_path}' está vazio ou não contém dados.\")\n",
    "    \n",
    "    except KeyError as e:\n",
    "        print(f\"Key Error: {e}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erro inesperado: {e}\")\n",
    "\n",
    "\n",
    "dados_limiares = read_and_order_ds(\"limiares.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função para calcular a distância euclidiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distancia_euclidiana(pontos):\n",
    "    \"\"\"\n",
    "    Calcula a distância euclidiana entre todos os pares de pontos fornecidos.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - pontos (list ou np.ndarray): Lista ou array numpy contendo as coordenadas dos pontos.\n",
    "    \n",
    "    Retorna:\n",
    "    - arestas_euclidiana (list): Lista de tuplas contendo os pares de pontos e suas respectivas distâncias.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Verifica se pontos é uma lista ou um array numpy\n",
    "        if not isinstance(pontos, (list, np.ndarray)):\n",
    "            raise TypeError(\"O parâmetro 'pontos' deve ser uma lista ou um array numpy.\")\n",
    "        \n",
    "        # Verifica se há pelo menos dois pontos\n",
    "        if len(pontos) < 2:\n",
    "            raise ValueError(\"É necessário pelo menos dois pontos para calcular a distância.\")\n",
    "        \n",
    "        # Verifica se cada ponto tem a mesma dimensão\n",
    "        dim_ponto = len(pontos[0])\n",
    "        if not all(len(p) == dim_ponto for p in pontos):\n",
    "            raise ValueError(\"Todos os pontos devem ter a mesma dimensão.\")\n",
    "        \n",
    "        np.array(pontos)  # Converte para array numpy, se necessário\n",
    "        arestas_euclidiana = []\n",
    "        tamanho_pontos = len(pontos)\n",
    "\n",
    "        for i in range(tamanho_pontos):\n",
    "            for j in range(i + 1, tamanho_pontos):\n",
    "                # Distância Euclidiana\n",
    "                ponto_i = np.array(pontos[i])\n",
    "                ponto_j = np.array(pontos[j])\n",
    "                distancia = np.linalg.norm(ponto_i - ponto_j)\n",
    "                arestas_euclidiana.append((i, j, distancia))\n",
    "\n",
    "        return arestas_euclidiana\n",
    "\n",
    "    except TypeError as e:\n",
    "        print(f\"Erro de tipo: {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Erro de valor: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro inesperado: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste 1: Pontos 2D simples (caso básico)\n",
    "pontos = np.array([[0, 0], [1, 1], [2, 2], [3, 3]])\n",
    "arestas_euclidiana = distancia_euclidiana(pontos)\n",
    "print(\"Teste 1: Pontos 2D simples\")\n",
    "for aresta in arestas_euclidiana:\n",
    "    print(f'Pontos {aresta[0]} e {aresta[1]}: Distância Euclidiana = {aresta[2]}')\n",
    "print()\n",
    "\n",
    "# Teste 2: Pontos 3D\n",
    "pontos = np.array([[0, 0, 0], [1, 1, 1], [2, 2, 2], [3, 3, 3]])\n",
    "arestas_euclidiana = distancia_euclidiana(pontos)\n",
    "print(\"Teste 2: Pontos 3D\")\n",
    "for aresta in arestas_euclidiana:\n",
    "    print(f'Pontos {aresta[0]} e {aresta[1]}: Distância Euclidiana = {aresta[2]}')\n",
    "print()\n",
    "\n",
    "# Teste 3: Apenas dois pontos\n",
    "pontos = np.array([[0, 0], [3, 4]])\n",
    "arestas_euclidiana = distancia_euclidiana(pontos)\n",
    "print(\"Teste 3: Apenas dois pontos\")\n",
    "for aresta in arestas_euclidiana:\n",
    "    print(f'Pontos {aresta[0]} e {aresta[1]}: Distância Euclidiana = {aresta[2]}')\n",
    "print()\n",
    "\n",
    "# Teste 4: Pontos com valores negativos\n",
    "pontos = np.array([[-1, -1], [0, 0], [1, 1]])\n",
    "arestas_euclidiana = distancia_euclidiana(pontos)\n",
    "print(\"Teste 4: Pontos com valores negativos\")\n",
    "for aresta in arestas_euclidiana:\n",
    "    print(f'Pontos {aresta[0]} e {aresta[1]}: Distância Euclidiana = {aresta[2]}')\n",
    "print()\n",
    "\n",
    "# Teste 5: Entrada inválida - Menos de dois pontos\n",
    "try:\n",
    "    pontos = np.array([[0, 0]])\n",
    "    arestas_euclidiana = distancia_euclidiana(pontos)\n",
    "except Exception as e:\n",
    "    print(f\"Teste 5: Menos de dois pontos - Erro: {e}\")\n",
    "print()\n",
    "\n",
    "# Teste 6: Entrada inválida - Dimensões diferentes entre pontos\n",
    "try:\n",
    "    pontos = np.array([[0, 0], [1, 1, 1]])\n",
    "    arestas_euclidiana = distancia_euclidiana(pontos)\n",
    "except Exception as e:\n",
    "    print(f\"Teste 6: Pontos com dimensões diferentes - Erro: {e}\")\n",
    "print()\n",
    "\n",
    "# Teste 7: Pontos vazios\n",
    "try:\n",
    "    pontos = np.array([])\n",
    "    arestas_euclidiana = distancia_euclidiana(pontos)\n",
    "except Exception as e:\n",
    "    print(f\"Teste 7: Pontos vazios - Erro: {e}\")\n",
    "print()\n",
    "\n",
    "# Teste 8: Pontos com valores decimais\n",
    "pontos = np.array([[0.5, 1.5], [2.3, 3.7], [4.1, 5.9]])\n",
    "arestas_euclidiana = distancia_euclidiana(pontos)\n",
    "print(\"Teste 8: Pontos com valores decimais\")\n",
    "for aresta in arestas_euclidiana:\n",
    "    print(f'Pontos {aresta[0]} e {aresta[1]}: Distância Euclidiana = {aresta[2]}')\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criação da extração dos dados do dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_info(idx_dataset, dataset_limiar):\n",
    "    \"\"\"\n",
    "    Extrai informações do dataset, como coordenadas, número de nós, e limiar.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - idx_dataset (int): Índice do dataset no DataFrame.\n",
    "    - dataset_limiar (pd.DataFrame): DataFrame contendo informações dos datasets.\n",
    "\n",
    "    Retorna:\n",
    "    - cordenadas (list): Lista de coordenadas dos nós.\n",
    "    - numero_de_nos (int): Número de nós no grafo.\n",
    "    - limiar (int ou float): Valor do limiar.\n",
    "    - name_dataset (str): Nome do dataset.\n",
    "\n",
    "    Lança:\n",
    "    - ValueError: Para problemas na leitura ou inconsistências no arquivo.\n",
    "    - KeyError: Para colunas ausentes no DataFrame.\n",
    "    - FileNotFoundError: Se o arquivo .tsp não for encontrado.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Valida se as colunas necessárias estão presentes no DataFrame\n",
    "        for coluna in ['Dataset', 'Nós', 'Limiar']:\n",
    "            if coluna not in dataset_limiar.columns:\n",
    "                raise KeyError(f\"A coluna '{coluna}' não foi encontrada no DataFrame.\")\n",
    "\n",
    "        # Verifica se o índice está dentro dos limites do DataFrame\n",
    "        if not (0 <= idx_dataset < len(dataset_limiar)):\n",
    "            raise IndexError(f\"O índice {idx_dataset} está fora dos limites do DataFrame.\")\n",
    "\n",
    "        # Obtém o nome do dataset e o caminho do arquivo .tsp\n",
    "        name_dataset = dataset_limiar.loc[idx_dataset, 'Dataset']\n",
    "        tsp_file_path = f\"datasets/{name_dataset}.tsp\"\n",
    "\n",
    "        # Tenta abrir o arquivo .tsp\n",
    "        try:\n",
    "            with open(tsp_file_path, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"O arquivo '{tsp_file_path}' não foi encontrado.\")\n",
    "\n",
    "        # Verifica se a seção NODE_COORD_SECTION existe no arquivo\n",
    "        try:\n",
    "            coord_section_index = lines.index('NODE_COORD_SECTION\\n')\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"A seção 'NODE_COORD_SECTION' não foi encontrada no arquivo '{tsp_file_path}'.\")\n",
    "\n",
    "        # Criando um grafo direcionado\n",
    "        G = nx.Graph()\n",
    "\n",
    "        # Adicionando nós e coordenadas ao grafo\n",
    "        for line in lines[coord_section_index + 1:]:\n",
    "            # Ignorar a linha EOF\n",
    "            if line.strip().lower() == 'eof':\n",
    "                break\n",
    "\n",
    "            node_info = line.split()\n",
    "\n",
    "            # Valida se a linha contém pelo menos 3 elementos\n",
    "            if len(node_info) < 3:\n",
    "                raise ValueError(f\"Dados inválidos no arquivo '{tsp_file_path}': {line.strip()}\")\n",
    "\n",
    "            # Converte os dados para o formato esperado\n",
    "            node_id = int(node_info[0])\n",
    "            x_coord = float(node_info[1])\n",
    "            y_coord = float(node_info[2])\n",
    "            G.add_node(node_id, pos=(x_coord, y_coord))\n",
    "\n",
    "        # Extrair apenas as coordenadas\n",
    "        cordenadas = [pos for _, pos in G.nodes(data='pos')]\n",
    "\n",
    "        # Obtém o número de nós e verifica se é válido\n",
    "        numero_de_nos = int(dados_limiares.loc[idx_dataset, 'Nós'])\n",
    "        if numero_de_nos <= 0:\n",
    "            raise ValueError(f\"O número de nós '{numero_de_nos}' deve ser maior que zero.\")\n",
    "\n",
    "        # Processa o limiar\n",
    "        limiar_raw = dados_limiares.loc[idx_dataset, 'Limiar']\n",
    "        if '[' in str(limiar_raw):\n",
    "            try:\n",
    "                # Avalia a string como uma lista\n",
    "                lista_valores = ast.literal_eval(limiar_raw)\n",
    "                if not isinstance(lista_valores, list) or not lista_valores:\n",
    "                    raise ValueError(f\"O limiar '{limiar_raw}' não é uma lista válida.\")\n",
    "                limiar = lista_valores[0]\n",
    "            except Exception:\n",
    "                raise ValueError(f\"O limiar '{limiar_raw}' não pôde ser avaliado como uma lista.\")\n",
    "        else:\n",
    "            try:\n",
    "                # Converte diretamente para inteiro ou float\n",
    "                limiar = int(limiar_raw)\n",
    "            except ValueError:\n",
    "                limiar = float(limiar_raw)\n",
    "\n",
    "        return cordenadas, numero_de_nos, limiar, name_dataset\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"Erro de chave: {e}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Erro de arquivo: {e}\")\n",
    "    except IndexError as e:\n",
    "        print(f\"Erro de índice: {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Erro de valor: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro inesperado: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_dataset = 0\n",
    "cordenadas, numero_de_nos, limiar,  name_dataset = dataset_info(idx_dataset, dados_limiares)\n",
    "print(f\"\"\"\n",
    "Informações do Dataset:\n",
    "------------------------\n",
    "- Nome do Dataset: {name_dataset}\n",
    "- Número de Nós  : {numero_de_nos}\n",
    "- Limiar         : {limiar}\n",
    "\n",
    "Coordenadas dos Nós:\n",
    "{cordenadas}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problema: Caixeiro Viajante (TSP - Travelling Salesman Problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algortimo: Branch and bound (Solução ótima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    Representa um nó na árvore do algoritmo Branch and Bound.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bound, nivel, custo, solucao):\n",
    "        \"\"\"\n",
    "        Inicializa um nó com os seguintes atributos:\n",
    "        \n",
    "        Parâmetros:\n",
    "        - bound (float): Limite associado ao nó na árvore.\n",
    "        - nivel (int): Nível do nó na árvore.\n",
    "        - custo (float): Custo acumulado até o nó atual.\n",
    "        - solucao (list): Solução parcial associada ao nó.\n",
    "        \"\"\"\n",
    "        self.bound = bound\n",
    "        self.nivel = nivel\n",
    "        self.custo = custo\n",
    "        self.solucao = solucao\n",
    "\n",
    "    def __lt__(self, other_node):\n",
    "        \"\"\"\n",
    "        Utilizado para determinar a ordem de prioridade em uma fila de prioridade.\n",
    "        Nós com menor bound têm maior prioridade.\n",
    "\n",
    "        Parâmetros:\n",
    "        - other_node (Node): Outro nó a ser comparado.\n",
    "        \n",
    "        Retorna:\n",
    "        - bool: True se o bound do nó atual for menor que o bound do outro nó.\n",
    "        \"\"\"\n",
    "        return self.bound < other_node.bound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bound(G, nodes):\n",
    "    \"\"\"\n",
    "    Calcula um limite inferior para a solução do problema TSP (Travelling Salesman Problem).\n",
    "    \n",
    "    Este limite é baseado na soma das arestas visitadas e nas menores arestas incidentes\n",
    "    necessárias para completar o caminho no grafo.\n",
    "\n",
    "    Parâmetros:\n",
    "    - G (networkx.Graph): Grafo representando o problema TSP.\n",
    "    - nodes (list): Lista de nós visitados na solução parcial.\n",
    "\n",
    "    Retorna:\n",
    "    - int: Um valor estimado para o custo total da solução, arredondado para cima.\n",
    "    \"\"\"\n",
    "\n",
    "    # Inicializa a estimativa total\n",
    "    estimativa = 0\n",
    "\n",
    "    # Converte a lista de vértices visitados em uma lista de arestas\n",
    "    size_nodes = len(nodes)\n",
    "    edges = [(nodes[i], nodes[i+1]) for i in range(size_nodes - 1)]\n",
    "\n",
    "    # Calcula o custo total das arestas na solução parcial (considerando o dobro do peso de cada aresta)\n",
    "    for (u, v) in edges:\n",
    "        estimativa += (2 * G[u][v]['weight'])\n",
    "\n",
    "    # Itera sobre todos os vértices do grafo para considerar arestas não visitadas\n",
    "    for u in G.nodes():\n",
    "        # Filtra as arestas que já foram usadas na solução parcial\n",
    "        arestas_u = list(filter(lambda x: x[0] == u or x[1] == u, edges))\n",
    "\n",
    "        # Verifica se o vértice 'u' tem menos de duas arestas incidentes na solução parcial\n",
    "        size_arestas_u = len(arestas_u)\n",
    "        if size_arestas_u < 2:\n",
    "            # Obtém as arestas incidentes ao vértice 'u' e ordena pelo peso\n",
    "            dados_u = list(G.edges(u, data=True))\n",
    "            dados_u = sorted(dados_u, key=lambda x: x[2]['weight'])\n",
    "            w1, w2 = dados_u[0][2]['weight'], dados_u[1][2]['weight']\n",
    "\n",
    "            # Caso 'u' não tenha arestas incidentes, adiciona os dois menores pesos\n",
    "            if size_arestas_u == 0:\n",
    "                estimativa += w1 + w2\n",
    "\n",
    "            # Caso 'u' tenha uma única aresta incidente, adiciona o menor peso restante\n",
    "            elif size_arestas_u == 1:\n",
    "                u, v = arestas_u[0]\n",
    "                peso_necessario = G[u][v]['weight']\n",
    "                estimativa += w1 if w1 < peso_necessario else w2\n",
    "\n",
    "    # Divide a estimativa por 2 e arredonda para cima (TSP considera ida e volta)\n",
    "    output = ceil(estimativa / 2)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeout(MAX_TIME_SEG)\n",
    "def branch_and_bound(grafo):\n",
    "    \"\"\"\n",
    "    Resolve o problema do Caixeiro Viajante (TSP) utilizando o algoritmo Branch and Bound.\n",
    "\n",
    "    Este método explora soluções parciais em uma árvore de decisão, utilizando limites (bounds)\n",
    "    para descartar caminhos que não podem resultar em soluções ótimas.\n",
    "\n",
    "    Parâmetros:\n",
    "    - grafo (networkx.Graph): Grafo representando o problema TSP com os pesos das arestas.\n",
    "\n",
    "    Retorna:\n",
    "    - solucao_encontrada (list): Lista de nós representando o caminho do Caixeiro Viajante.\n",
    "    - melhor_solucao (float): Custo total do caminho encontrado.\n",
    "    - espaco_gasto_mb (float): Memória máxima usada durante a execução, em megabytes.\n",
    "    \"\"\"\n",
    "    tracemalloc.start()  # Inicia o rastreamento de alocação de memória\n",
    "    G = grafo.copy()  # Faz uma cópia do grafo original para preservar o estado\n",
    "\n",
    "    # Cria o nó raiz da árvore do Branch and Bound\n",
    "    raiz = Node(bound(G, []), 1, 0, [0])  # Raiz começa no nó 0 com solução parcial vazia\n",
    "\n",
    "    # Inicializa a fila de prioridade com o nó raiz\n",
    "    fila_prioridade = PriorityQueue()\n",
    "    fila_prioridade.put(raiz)\n",
    "\n",
    "    # Inicializa variáveis para armazenar a melhor solução encontrada\n",
    "    melhor_solucao = np.inf  # Começa com um valor infinito como pior caso\n",
    "    solucao_encontrada = []\n",
    "\n",
    "    # Processa a fila de prioridade enquanto houver nós a explorar\n",
    "    while not fila_prioridade.empty():\n",
    "        # Obtém o próximo nó com menor bound\n",
    "        no = fila_prioridade.get()\n",
    "\n",
    "        # Verifica se o nó atual é uma folha (solução completa)\n",
    "        if no.nivel == G.number_of_nodes():\n",
    "            # Calcula o custo total, incluindo o retorno ao nó inicial\n",
    "            custo_total = no.custo + G[no.solucao[-1]][0]['weight']\n",
    "            \n",
    "            # Atualiza a melhor solução, se o custo atual for menor\n",
    "            if custo_total < melhor_solucao:\n",
    "                melhor_solucao = custo_total\n",
    "                solucao_encontrada = no.solucao + [0]  # Adiciona o nó inicial ao final\n",
    "        # Explora nós filhos se o bound for promissor e o nó não for uma folha\n",
    "        elif no.bound < melhor_solucao and no.nivel < G.number_of_nodes():\n",
    "            v = no.solucao[-1]  # Último nó da solução parcial atual\n",
    "\n",
    "            # Gera nós filhos para todos os nós não visitados\n",
    "            for k in range(G.number_of_nodes()):\n",
    "                if k not in no.solucao and G.has_edge(v, k):\n",
    "                    nova_solucao = no.solucao + [k]\n",
    "                    estimativa = bound(G, nova_solucao)\n",
    "\n",
    "                    # Adiciona o nó filho à fila de prioridade se sua estimativa for válida\n",
    "                    if estimativa < melhor_solucao:\n",
    "                        novo_custo = no.custo + G[v][k]['weight']\n",
    "                        fila_prioridade.put(Node(estimativa, no.nivel + 1, novo_custo, nova_solucao))\n",
    "\n",
    "    # Mede o pico de memória usado durante a execução\n",
    "    _, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()  # Finaliza o rastreamento de memória\n",
    "\n",
    "    espaco_gasto_mb = peak / (1024 ** 2)  # Converte bytes para megabytes\n",
    "\n",
    "    # Retorna a melhor solução encontrada, o custo total e o uso de memória\n",
    "    return solucao_encontrada, melhor_solucao, espaco_gasto_mb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_dataset = 0\n",
    "cordenadas, numero_de_nos, limiar, name_dataset = dataset_info(idx_dataset, dados_limiares)\n",
    "print(f\"Coordenadas: {cordenadas}\")\n",
    "print(f\"Número de nós: {numero_de_nos}\")\n",
    "print(f\"Limiar: {limiar}\")\n",
    "print(f\"Nome do dataset: {name_dataset}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Christofides (1.5 aproximado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeout(MAX_TIME_SEG)\n",
    "def christofides(grafo):\n",
    "    \"\"\"\n",
    "    Resolve o problema do Caixeiro Viajante (TSP) usando o algoritmo de Christofides.\n",
    "\n",
    "    Este algoritmo aproxima a solução ótima em no máximo 1.5 vezes o custo ótimo,\n",
    "    utilizando árvores geradoras mínimas (MST), emparelhamentos perfeitos de peso mínimo\n",
    "    e circuitos Eulerianos.\n",
    "\n",
    "    Parâmetros:\n",
    "    - grafo (networkx.Graph): Grafo representando o problema TSP com os pesos das arestas.\n",
    "\n",
    "    Retorna:\n",
    "    - caminho (list): Lista de nós representando o caminho aproximado do Caixeiro Viajante.\n",
    "    - comprimento (float): Custo total do caminho encontrado.\n",
    "    - espaco_gasto_mb (float): Memória máxima usada durante a execução, em megabytes.\n",
    "    \"\"\"\n",
    "    tracemalloc.start()  # Inicia o rastreamento de memória\n",
    "    G = grafo.copy()  # Cria uma cópia do grafo original\n",
    "\n",
    "    # Passo 1: Encontrar a Árvore Geradora Mínima (MST) do grafo\n",
    "    AGM = nx.minimum_spanning_tree(G)\n",
    "\n",
    "    # Passo 2: Identificar vértices com grau ímpar na MST\n",
    "    vertices_grau_impar = [vertice for vertice in AGM.nodes() if AGM.degree(vertice) % 2 == 1]\n",
    "\n",
    "    # Criar um subgrafo induzido pelos vértices de grau ímpar\n",
    "    grafo_induzido = G.subgraph(vertices_grau_impar)\n",
    "\n",
    "    # Passo 3: Emparelhamento perfeito de peso mínimo\n",
    "    # Inverte o peso das arestas no subgrafo induzido para encontrar o emparelhamento perfeito de peso mínimo\n",
    "    for u, v in grafo_induzido.edges():\n",
    "        grafo_induzido[u][v]['weight'] *= -1\n",
    "\n",
    "    # Encontra o emparelhamento perfeito de peso mínimo (revertendo os pesos ao final)\n",
    "    emparelhamento_min_peso = nx.max_weight_matching(grafo_induzido, maxcardinality=True)\n",
    "\n",
    "    # Subgrafo induzido pelas arestas do emparelhamento perfeito de peso mínimo\n",
    "    grafo_emparelhamento_min_peso = G.edge_subgraph(emparelhamento_min_peso)\n",
    "\n",
    "    # Passo 4: Construção do Multigrafo\n",
    "    # Cria um multigrafo contendo as arestas da MST e do emparelhamento perfeito de peso mínimo\n",
    "    multigrafo = nx.MultiGraph()\n",
    "    multigrafo.add_weighted_edges_from(AGM.edges.data('weight'))\n",
    "    multigrafo.add_weighted_edges_from(grafo_emparelhamento_min_peso.edges.data('weight'))\n",
    "\n",
    "    # Passo 5: Encontrar o circuito Euleriano no multigrafo\n",
    "    circuito_euleriano = [u for u, v in nx.eulerian_circuit(multigrafo, source=0)]\n",
    "\n",
    "    # Passo 6: Construir o circuito Hamiltoniano a partir do Euleriano\n",
    "    # Remove vértices duplicados para obter o caminho Hamiltoniano\n",
    "    caminho = list(dict.fromkeys(circuito_euleriano))\n",
    "    caminho.append(0)  # Adiciona o nó inicial ao final para fechar o circuito\n",
    "\n",
    "    # Calcula o comprimento total do caminho encontrado\n",
    "    comprimento = sum(grafo[u][v]['weight'] for u, v in zip(caminho, caminho[1:]))\n",
    "\n",
    "    # Mede o pico de uso de memória durante a execução\n",
    "    _, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()  # Finaliza o rastreamento de memória\n",
    "\n",
    "    espaco_gasto_mb = peak / (1024 ** 2)  # Converte bytes para megabytes\n",
    "\n",
    "    # Retorna o caminho encontrado, o custo total e o uso de memória\n",
    "    return caminho, comprimento, espaco_gasto_mb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### twice around the tree (2 aproximado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeout(MAX_TIME_SEG)\n",
    "def twice_around_the_tree(grafo):\n",
    "    \"\"\"\n",
    "    Resolve o problema do Caixeiro Viajante (TSP) usando o algoritmo \"Twice Around the Tree\".\n",
    "\n",
    "    Este algoritmo aproxima a solução ótima em no máximo 2 vezes o custo ótimo.\n",
    "    Baseia-se em encontrar a Árvore Geradora Mínima (MST) e realizar um percurso em pré-ordem\n",
    "    para gerar uma solução aproximada.\n",
    "\n",
    "    Parâmetros:\n",
    "    - grafo (networkx.Graph): Grafo representando o problema TSP com os pesos das arestas.\n",
    "\n",
    "    Retorna:\n",
    "    - caminho (list): Lista de nós representando o caminho aproximado do Caixeiro Viajante.\n",
    "    - comprimento (float): Custo total do caminho encontrado.\n",
    "    - espaco_gasto_mb (float): Memória máxima usada durante a execução, em megabytes.\n",
    "    \"\"\"\n",
    "    tracemalloc.start()  # Inicia o rastreamento de memória\n",
    "    G = grafo.copy()  # Cria uma cópia do grafo original\n",
    "\n",
    "    # Passo 1: Encontrar a Árvore Geradora Mínima (MST) do grafo\n",
    "    arvore_geradora_minima = nx.minimum_spanning_tree(G)\n",
    "\n",
    "    # Passo 2: Realizar um percurso em pré-ordem na MST\n",
    "    # Inicia o percurso pelo vértice 0 (nó raiz)\n",
    "    caminho = list(nx.dfs_preorder_nodes(arvore_geradora_minima, source=0))\n",
    "    caminho.append(0)  # Fecha o circuito adicionando o nó inicial ao final\n",
    "\n",
    "    # Passo 3: Calcular o comprimento do caminho encontrado\n",
    "    comprimento = sum(G[u][v]['weight'] for u, v in zip(caminho, caminho[1:]))\n",
    "\n",
    "    # Mede o pico de uso de memória durante a execução\n",
    "    _, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()  # Finaliza o rastreamento de memória\n",
    "\n",
    "    espaco_gasto_mb = peak / (1024 ** 2)  # Converte bytes para megabytes\n",
    "\n",
    "    # Retorna o caminho encontrado, o custo total e o uso de memória\n",
    "    return caminho, comprimento, espaco_gasto_mb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitorar_recursos(intervalo, resultados):\n",
    "    \"\"\"\n",
    "    Função que monitora o uso de CPU, memória e disco a cada 'intervalo' segundos.\n",
    "    Armazena os resultados em uma lista fornecida.\n",
    "    \"\"\"\n",
    "    while resultados['executando']:\n",
    "        cpu_percent = psutil.cpu_percent(interval=0.1)\n",
    "        ram_usage = psutil.virtual_memory().percent\n",
    "        disk_usage = psutil.disk_usage('/').percent\n",
    "        \n",
    "        resultados['cpu'].append(cpu_percent)\n",
    "        resultados['ram'].append(ram_usage)\n",
    "        resultados['disco'].append(disk_usage)\n",
    "\n",
    "        time.sleep(intervalo)\n",
    "\n",
    "def analise_dados(idx_dataset, df_dados_dataset, dataset_info, alg):\n",
    "    \"\"\"\n",
    "    Realiza a análise do dataset e executa o algoritmo especificado (Branch and Bound, Christofides ou Twice Around the Tree).\n",
    "    Monitora o uso de CPU, RAM, disco e GPU durante a execução.\n",
    "    \"\"\"\n",
    "    # Extraindo informações do dataset\n",
    "    cordenadas, numero_de_nos, limiar, name_dataset = dataset_info(idx_dataset, df_dados_dataset)\n",
    "\n",
    "    # Criando o dicionário que mapeia algoritmos às suas funções\n",
    "    algoritmos = {\n",
    "        \"bb\": (branch_and_bound, 1),  # Branch and Bound, fator de aproximação 1\n",
    "        \"chr\": (christofides, 1.5),  # Christofides, fator de aproximação 1.5\n",
    "        \"tat\": (twice_around_the_tree, 2)  # Twice Around the Tree, fator de aproximação 2\n",
    "    }\n",
    "\n",
    "    if alg not in algoritmos:\n",
    "        raise ValueError(f\"Algoritmo '{alg}' não reconhecido. Use 'bb', 'chr' ou 'tat'.\")\n",
    "\n",
    "    funcao_algoritmo, apro = algoritmos[alg]\n",
    "\n",
    "    arestas_euclidiana = distancia_euclidiana(cordenadas)\n",
    "    G = nx.Graph()\n",
    "    G.add_weighted_edges_from(arestas_euclidiana)\n",
    "\n",
    "    # Iniciando monitoramento de recursos\n",
    "    resultados = {'cpu': [], 'ram': [], 'disco': [], 'gpu': [], 'executando': True}\n",
    "    intervalo_monitoramento = 0.5  # Intervalo de 0.5 segundos entre amostragens\n",
    "\n",
    "    thread_cpu_ram = threading.Thread(target=monitorar_recursos, args=(intervalo_monitoramento, resultados))\n",
    "\n",
    "    thread_cpu_ram.start()\n",
    "\n",
    "    # Executando o algoritmo e medindo o tempo\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        _, comprimento, espaco_gasto_mb = funcao_algoritmo(G)\n",
    "    except TimeoutError:\n",
    "        comprimento, espaco_gasto_mb = \"NA\", \"NA\"\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Finalizando o monitoramento\n",
    "    resultados['executando'] = False\n",
    "    thread_cpu_ram.join()\n",
    "\n",
    "    tempo_execucao = end_time - start_time\n",
    "\n",
    "    # Calculando médias de uso de recursos\n",
    "    media_cpu = np.mean(resultados['cpu'])\n",
    "    media_ram = np.mean(resultados['ram'])\n",
    "    media_disco = np.mean(resultados['disco'])\n",
    "\n",
    "    taxa_de_aproximacao = comprimento / limiar if comprimento != \"NA\" else \"NA\"\n",
    "    valor_max_esperado = apro * limiar if comprimento != \"NA\" else \"NA\"\n",
    "\n",
    "    # Retornando os resultados da análise\n",
    "    return (\n",
    "        limiar, \n",
    "        valor_max_esperado, \n",
    "        comprimento, \n",
    "        taxa_de_aproximacao, \n",
    "        numero_de_nos, \n",
    "        name_dataset, \n",
    "        tempo_execucao, \n",
    "        espaco_gasto_mb,\n",
    "        media_cpu,\n",
    "        media_ram,\n",
    "        media_disco\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criando datasets com a analise dos algoritmos branch-and-bound, o algoritmo twice-around-the-tree, e o algoritmo de Christofides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analise_completa(dados_limiares, dataset_info, alg, nome_arquivo):\n",
    "    \"\"\"\n",
    "    Realiza a análise completa de um dataset usando o algoritmo especificado e salva os resultados em um arquivo.\n",
    "\n",
    "    Parâmetros:\n",
    "    - dados_limiares (pd.DataFrame): DataFrame contendo informações sobre os datasets, incluindo limites e características.\n",
    "    - dataset_info (function): Função para extrair informações detalhadas de um dataset específico.\n",
    "    - alg (str): Algoritmo a ser utilizado. Valores possíveis:\n",
    "        - \"chr\" para Christofides.\n",
    "        - \"tat\" para Twice Around the Tree.\n",
    "        - \"bb\" para Branch and Bound.\n",
    "    - nome_arquivo (str): Caminho e nome do arquivo onde os resultados serão salvos.\n",
    "\n",
    "    Retorna:\n",
    "    - None. Os resultados são salvos diretamente no arquivo especificado.\n",
    "    \"\"\"\n",
    "\n",
    "    # Dicionário que mapeia os nomes abreviados dos algoritmos para seus nomes completos\n",
    "    nome_algoritmos = {\n",
    "        \"chr\": \"christofides\",\n",
    "        \"tat\": \"twice_around_the_tree\",\n",
    "        \"bb\": \"branch_and_bound\"\n",
    "    }\n",
    "\n",
    "    # Validação do algoritmo fornecido\n",
    "    if alg not in nome_algoritmos:\n",
    "        raise ValueError(f\"Algoritmo '{alg}' não reconhecido. Use 'chr', 'tat' ou 'bb'.\")\n",
    "\n",
    "    # Nome completo do algoritmo para inclusão no DataFrame\n",
    "    algoritmo_name = nome_algoritmos[alg]\n",
    "\n",
    "    # Criação de uma cópia do DataFrame original para manipulação\n",
    "    df = dados_limiares.copy()\n",
    "\n",
    "    # Adiciona colunas necessárias ao DataFrame para armazenar os resultados da análise\n",
    "    colunas_novas = [\n",
    "        \"algoritmo\", \n",
    "        \"tempo execução\", \n",
    "        \"taxa aproximação atingida\", \n",
    "        \"custo caminho atingido\", \n",
    "        \"Espaço (MB) usado\"\n",
    "    ]\n",
    "    df[colunas_novas] = pd.NA\n",
    "\n",
    "    # Preenche a coluna de algoritmo com o nome correspondente\n",
    "    df[\"algoritmo\"] = df[\"algoritmo\"].fillna(algoritmo_name)\n",
    "    df_recursos = []  # Lista para armazenar dados de uso de recursos\n",
    "\n",
    "    # Salva o DataFrame inicial no arquivo para registrar a estrutura\n",
    "    df.to_csv(nome_arquivo, sep='\\t', index=False)\n",
    "\n",
    "    # Itera sobre cada linha do DataFrame para realizar a análise\n",
    "    for idx, row in df.iterrows():\n",
    "        # Realiza a análise detalhada para o índice atual\n",
    "        limiar, valor_max_esperado, comprimento, taxa_de_aproximacao, numero_de_nos, name_dataset, tempo_execucao, espaco_gasto_mb, media_cpu, media_ram, media_disco = analise_dados(\n",
    "            idx, dados_limiares, dataset_info, alg\n",
    "        )\n",
    "\n",
    "        # Verifica se o dataset atual corresponde à linha do DataFrame\n",
    "        if name_dataset == row['Dataset']:\n",
    "            # Atualiza as colunas de resultado para o índice atual\n",
    "            df.at[idx, \"tempo execução\"] = tempo_execucao\n",
    "            df.at[idx, \"taxa aproximação atingida\"] = taxa_de_aproximacao\n",
    "            df.at[idx, \"custo caminho atingido\"] = comprimento\n",
    "            df.at[idx, \"Espaço (MB) usado\"] = espaco_gasto_mb\n",
    "            df.at[idx, \"CPU (%)\"] = media_cpu\n",
    "            df.at[idx, \"RAM (%)\"] = media_ram\n",
    "            df.at[idx, \"Disco (%)\"] = media_disco\n",
    "\n",
    "        # Salva o DataFrame atualizado no arquivo a cada iteração (para maior segurança)\n",
    "        df.to_csv(nome_arquivo, sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def realizar_analise_algoritmos(df_dados_dataset, dataset_info):\n",
    "    \"\"\"\n",
    "    Executa a análise completa para diferentes algoritmos de solução do TSP \n",
    "    (twice-around-the-tree, Christofides e Branch-and-Bound) e salva os resultados em arquivos.\n",
    "\n",
    "    Parâmetros:\n",
    "    - df_dados_dataset (pd.DataFrame): DataFrame contendo informações sobre os datasets.\n",
    "    - dataset_info (function): Função para extrair informações detalhadas de um dataset.\n",
    "\n",
    "    Retorna:\n",
    "    - None. Os resultados são salvos em arquivos específicos.\n",
    "    \"\"\"\n",
    "\n",
    "    # Dicionário mapeando algoritmos aos seus arquivos de saída correspondentes\n",
    "    algoritmos = {\n",
    "        \"tat\": \"analise_twice-around-the-tree.txt\",\n",
    "        \"chr\": \"analise_christofides.txt\",\n",
    "        \"bb\": \"analise_branch-and-bound.txt\"\n",
    "    }\n",
    "\n",
    "    for alg, nome_arquivo in algoritmos.items():\n",
    "        print(f\"\\nExecutando análise com o algoritmo '{alg}' e salvando em '{nome_arquivo}'...\\n\")\n",
    "        try:\n",
    "            # Executa a análise para o algoritmo atual\n",
    "            analise_completa(df_dados_dataset, dataset_info, alg, nome_arquivo)\n",
    "            print(f\"Análise com '{alg}' concluída e salva no arquivo '{nome_arquivo}'.\\n\")\n",
    "        except Exception as e:\n",
    "            # Trata erros durante a execução de qualquer algoritmo\n",
    "            print(f\"Erro ao executar análise com '{alg}': {e}\\n\")\n",
    "\n",
    "# Chama a função para realizar os testes\n",
    "realizar_analise_algoritmos(dados_limiares, dataset_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exibir_resultados_analise(arquivos_analise):\n",
    "    \"\"\"\n",
    "    Exibe os resultados das análises armazenadas em arquivos txt e retorna os DataFrames.\n",
    "\n",
    "    Parâmetros:\n",
    "    - arquivos_analise (dict): Dicionário contendo os nomes dos algoritmos como chaves \n",
    "      e os caminhos para os arquivos de análise como valores.\n",
    "\n",
    "    Retorna:\n",
    "    - dict: Um dicionário com os nomes dos algoritmos como chaves e os DataFrames carregados como valores.\n",
    "    \"\"\"\n",
    "    resultados = {}\n",
    "    for nome_algoritmo, caminho_arquivo in arquivos_analise.items():\n",
    "        try:\n",
    "            print(f\"\\nResultados para o algoritmo: {nome_algoritmo}\\n{'-' * 40}\")\n",
    "            df_resultados = pd.read_csv(caminho_arquivo, sep='\\t')\n",
    "            display(df_resultados)  # Exibe o DataFrame\n",
    "            resultados[nome_algoritmo] = df_resultados\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao exibir resultados do arquivo '{caminho_arquivo}': {e}\\n\")\n",
    "            resultados[nome_algoritmo] = None  # Marca como None se houve erro\n",
    "    return resultados\n",
    "\n",
    "# Exemplo de uso\n",
    "arquivos_analise = {\n",
    "    \"twice-around-the-tree\": \"analise_twice-around-the-tree.txt\",\n",
    "    \"christofides\": \"analise_christofides.txt\",\n",
    "    \"branch-and-bound\": \"analise_branch-and-bound.txt\"\n",
    "}\n",
    "\n",
    "resultados = exibir_resultados_analise(arquivos_analise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acessando os DataFrames retornados\n",
    "df_atat = resultados[\"twice-around-the-tree\"]\n",
    "df_achr = resultados[\"christofides\"]\n",
    "df_bb = resultados[\"branch-and-bound\"]\n",
    "\n",
    "# Substituindo valores nulos por 0\n",
    "df_atat = df_atat.fillna(0) if df_atat is not None else None\n",
    "df_achr = df_achr.fillna(0) if df_achr is not None else None\n",
    "df_bb = df_bb.fillna(0) if df_bb is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar gráfico\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(df_atat['Nós'], df_atat['tempo execução'], marker='o', color='blue', label='twice-around-the-tree')\n",
    "plt.plot(df_achr['Nós'], df_achr['tempo execução'], marker='o', color='green', label='christofide')\n",
    "plt.plot(df_bb['Nós'], df_bb['tempo execução'], marker='o', color='red', label='christofide')\n",
    "\n",
    "plt.title(\"Tempo de execução em relação ao número de nós\")\n",
    "plt.xlabel(\"Número de nós\")\n",
    "plt.ylabel(\"Tempo de execução\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\n",
    "    'dados/analise_branch-and-bound.txt',\n",
    "    'dados/analise_twice-around-the-tree.txt',\n",
    "    'dados/analise_christofides.txt'\n",
    "]\n",
    "\n",
    "# Reading data\n",
    "columns = [\"Dataset\", \"Nós\", \"Limiar\", \"Algoritmo\", \"Tempo de Execução\", \"Taxa Aproximação Atingida\", \"Custo Caminho Atingido\", \"Espaço (MB) Usado\", \"CPU (%)\", \"RAM (%)\", \"Disco (%)\"]\n",
    "dataframes = [pd.read_csv(file, sep=\"\\t\", header=0, names=columns, engine='python') for file in file_paths]\n",
    "\n",
    "# Merging datasets\n",
    "data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Cleaning and converting data\n",
    "data = data.dropna(subset=[\"Nós\", \"Taxa Aproximação Atingida\", \"Custo Caminho Atingido\", \"Espaço (MB) Usado\", \"CPU (%)\", \"RAM (%)\", \"Disco (%)\"])\n",
    "for col in [\"Nós\", \"Taxa Aproximação Atingida\", \"Custo Caminho Atingido\", \"Espaço (MB) Usado\", \"CPU (%)\", \"RAM (%)\", \"Disco (%)\"]:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"graficos\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Plotting function\n",
    "def plot_metric(data, metric, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for algo in data['Algoritmo'].unique():\n",
    "        subset = data[data['Algoritmo'] == algo]\n",
    "        plt.plot(subset['Nós'], subset[metric], label=algo)\n",
    "    plt.xlabel('Nós')\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save the plot\n",
    "    file_name = os.path.join(output_dir, f\"{title.replace(' ', '_')}.png\")\n",
    "    plt.savefig(file_name)\n",
    "    plt.close()\n",
    "\n",
    "# Metrics to plot\n",
    "metrics = [\"Taxa Aproximação Atingida\", \"Custo Caminho Atingido\", \"Espaço (MB) Usado\", \"CPU (%)\", \"RAM (%)\", \"Disco (%)\"]\n",
    "\n",
    "# Generate plots\n",
    "for metric in metrics:\n",
    "    plot_metric(data, metric, f'Gráfico de {metric} por Nós')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
